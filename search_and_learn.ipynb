{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xctusmJ6BZ6_"
   },
   "source": [
    "# Scaling Test-Time Compute for Longer Thinking in LLMs\n",
    "\n",
    "_Authored by: [Sergio Paniego](https://github.com/sergiopaniego)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmgoppItAO7B"
   },
   "source": [
    "üö® **WARNING**: This notebook is **resource-intensive** and requires substantial computational power. If you‚Äôre running this in **Colab**, it will utilize an **A100 GPU**.\n",
    "\n",
    "---\n",
    "\n",
    "In this recipe, we'll guide you through extending the inference time for an **Instruct LLM system** using **test-time compute** to solve more challenging problems, such as **complex math problems**. This approach, inspired by [**OpenAI o1-o3 models**](https://openai.com/index/learning-to-reason-with-llms/), demonstrates that **longer reasoning time** during inference can enhance model performance.\n",
    "\n",
    "This technique builds on experiments shared in [this **blog post**](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute), which show that smaller models, like the **1B** and **3B Llama Instruct models**, can outperform much larger ones on the **MATH-500 benchmark** when given enough **\"time to think\"**. Recent research from [DeepMind](https://arxiv.org/abs/2408.03314) suggests that **test-time compute** can be scaled optimally through strategies like iterative self-refinement or using a reward model.\n",
    "\n",
    "The blog introduces a [**new repository**](https://github.com/huggingface/search-and-learn) for running these experiments. In this recipe, we'll focus on building a **small chatbot** that engages in **longer reasoning** to tackle **harder problems** using small open models.\n",
    "\n",
    "![Instruct LLM Methodology](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/methods-thumbnail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twKCzVIg71Xa"
   },
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Let‚Äôs start by installing the [search-and-learn](https://github.com/huggingface/search-and-learn) repository! üöÄ  \n",
    "This repo is designed to replicate the experimental results and is not a Python pip package. However, we can still use it to generate our system. To do so, we‚Äôll need to install it from source with the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0YDC2_7XTm8"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/search-and-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd hack-search-and-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kT3jH_d_XcEb"
   },
   "outputs": [],
   "source": [
    "!pip install -e '.[dev]'\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAQHu9T176zh"
   },
   "source": [
    "Log in to Hugging Face to access [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as it is a gated model! üóùÔ∏è  \n",
    "If you haven't previously requested access, you'll need to submit a request before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pnEaTlFYZF_H"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4dd756970604521bb87666e3df00364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX07zCTA8MWL"
   },
   "source": [
    "## 2. Setup the Large Language Model (LLM) and the Process Reward Model (PRM) üí¨\n",
    "\n",
    "As illustrated in the diagram, the system consists of an LLM that generates intermediate answers based on user input, a [PRM model](https://huggingface.co/papers/2211.14275) that evaluates and scores these answers, and a search strategy that uses the PRM feedback to guide the subsequent steps in the search process until reaching the final answer.\n",
    "\n",
    "Let‚Äôs begin by initializing each model. For the LLM, we‚Äôll use the [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) model, and for the PRM, we‚Äôll use the [RLHFlow/Llama3.1-8B-PRM-Deepseek-Data](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkJw0x7gDJEY"
   },
   "source": [
    "![system](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/system.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_src = \"src/\"\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.append(project_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG1MolfxmZ7M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM\n",
    "from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "model_path=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "prm_path=\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
    "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
    "    seed=42,                     # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "prm = RLHFFlow(prm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYtPn0_V_YRx"
   },
   "source": [
    "### 2.1 Instantiate the Question, Search Strategy, and Call the Pipeline\n",
    "\n",
    "Now that we've set up the LLM and PRM, let's proceed by defining the question, selecting a search strategy to retrieve relevant information, and calling the pipeline to process the question through the models.\n",
    "\n",
    "1. **Instantiate the Question**: In this step, we define the input question that the system will answer, considering the given context.\n",
    "\n",
    "2. **Search Strategy**: The system currently supports the following search strategies: `best_of_n`, `beam_search`, and `dvts` (see diagram). For this example, we'll use `best_of_n`, but you can easily switch to any of the other strategies based on your needs. We need to define some configuration parameters for the configuration of the search strategy. You can check the full list [here](https://github.com/huggingface/search-and-learn/blob/main/src/sal/config.py).\n",
    "\n",
    "3. **Call the Pipeline**: With the question and search strategy in place, we‚Äôll call the inference pipeline, processing the inputs through both the LLM and PRM to generate the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSWINPerJrhm"
   },
   "source": [
    "![](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/search-strategies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z69xD6i2L5a6"
   },
   "source": [
    "The first step is to clearly define the question that the system will answer. This ensures that we have a precise task for the model to tackle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83puLxhzsOM0"
   },
   "outputs": [],
   "source": [
    "question_text = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
    "input_batch = {\"problem\": [question_text]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGpyzMNkAO7H"
   },
   "source": [
    "Next, we define the configuration, including parameters like the number of candidate answers `(N)`, and choose the search strategy that will be used. The search strategy dictates how we explore the potential answers. In this case, we'll use `best_of_n`.\n",
    "\n",
    "With the question and configuration in place, we use the selected search strategy to generate multiple candidate answers. These candidates are evaluated based on their relevance and quality and the final answer is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6s6GS16QZLV"
   },
   "outputs": [],
   "source": [
    "from sal.config import Config\n",
    "from sal.search import beam_search, best_of_n, dvts\n",
    "\n",
    "config = Config()\n",
    "config.n=32 # Number of answers to generate during the search\n",
    "\n",
    "search_result = best_of_n(x=input_batch, config=config, llm=llm, prm=prm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsLHD_6C_15p"
   },
   "source": [
    "### 2.2 Display the Final Result\n",
    "\n",
    "Once the pipeline has processed the question through the LLM and PRM, we can display the final result. This result will be the model's output after considering the intermediate answers and scoring them using the PRM.\n",
    "\n",
    "Here's how to display the final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "v8medbURbgdI",
    "outputId": "3620f3e6-a25d-4bec-f41c-c4f03a6ed770"
   },
   "outputs": [],
   "source": [
    "search_result['pred'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-8hIu05AO7J"
   },
   "source": [
    "The model‚Äôs output might include special tokens, such as `<|start_header_id|>` or `<|end_header_id|>`. To make the answer more readable, we can safely remove them before displaying it to the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "flbIu6-rDapM",
    "outputId": "fcb197d5-0f21-4953-8a21-869c92a1f957"
   },
   "outputs": [],
   "source": [
    "formatted_output = search_result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
    "formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZuLZNirAO7J"
   },
   "source": [
    "After removing any special tokens, we can display the final answer to the user. Since the answer is based on markdown, it can be rendered properly by displaying it as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "P4En0qJRD0cl",
    "outputId": "56400fea-e304-4f16-d255-909f42f636e0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uCpYzAw_4o9"
   },
   "source": [
    "## 3. Assembling It All! üßë‚Äçüè≠Ô∏è\n",
    "\n",
    "Now, let's create a method that encapsulates the entire pipeline. This will allow us to easily reuse the process in future applications, making it efficient and modular.\n",
    "\n",
    "By combining the LLM, PRM, search strategy, and result display, we can simplify the workflow and ensure that it‚Äôs reusable for other tasks or questions.\n",
    "\n",
    "We simplify the workflow, ensuring that it‚Äôs reusable for different tasks or questions. Additionally, we‚Äôll track the time spent on each method so that we can **understand the practical implications** of using each strategy and configuration.\n",
    "\n",
    "Here‚Äôs how we can structure the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpswbcVi37KR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given question using the search-and-learn pipeline.\n",
    "\n",
    "    Args:\n",
    "    - question (str): The input question to generate an answer for.\n",
    "    - config (Config): Configuration object containing parameters for search strategy.\n",
    "    - llm (LLM): Pretrained large language model used for generating answers.\n",
    "    - prm (RLHFFlow): Process reward model used for evaluating answers.\n",
    "    - method (str): Search strategy to use. Options are 'best_of_n', 'beam_search', 'dvts'. Default is 'best_of_n'.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted output after processing the question.\n",
    "    \"\"\"\n",
    "    batch = {\"problem\": [question]}\n",
    "\n",
    "    start_time = time.time()\n",
    "    if method == 'best_of_n':\n",
    "      result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n",
    "    elif method == 'beam_search':\n",
    "      result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n",
    "    elif method == 'dvts':\n",
    "      result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    total_tokens = 0\n",
    "    for completion in result['completions']:\n",
    "        for comp in  completion:\n",
    "            output_tokens = tokenizer.encode(comp)\n",
    "            total_tokens += len(output_tokens)\n",
    "\n",
    "    print(f\"Total tokens in all completions: {total_tokens}\")\n",
    "\n",
    "    formatted_output = result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
    "    return formatted_output, elapsed_time, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWbOqkiKPVd2"
   },
   "source": [
    "### ‚è≥  3.1 Comparing Thinking Time for Each Strategy\n",
    "\n",
    "Let‚Äôs compare the **thinking time** of three methods: `best_of_n`, `beam_search`, and `dvts`. Each method is evaluated using the same number of answers during the search process, measuring the time spent thinking in seconds and the number of generated tokens.\n",
    "\n",
    "In the results below, the `best_of_n` method shows the least thinking time, while the `dvts` method takes the most time. However, `best_of_n` generates more tokens due to its simpler search strategy.\n",
    "\n",
    "| **Method**      | **Number of Answers During Search** | **Thinking Time (Seconds)** | **Generated Tokens** |\n",
    "|------------------|-------------------------------------|-----------------------------|-----------------------|\n",
    "| **best_of_n**    | 8                                   | 3.54                        | 3087                  |\n",
    "| **beam_search**  | 8                                   | 10.06                       | 2049                  |\n",
    "| **dvts**         | 8                                   | 8.46                        | 2544                  |\n",
    "\n",
    "This comparison illustrates the trade-offs between the strategies, balancing time spent thinking and the complexity of the search process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ROJwROGX8q-"
   },
   "source": [
    "#### 1. **Best of n**\n",
    "\n",
    "We‚Äôll begin by using the `best_of_n` strategy. Here‚Äôs how to track the thinking time for this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_fWKy5CCTLV",
    "outputId": "8d77eea3-b23e-4eba-cfe3-5935fae1405d"
   },
   "outputs": [],
   "source": [
    "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
    "\n",
    "config.n=8\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "uzKfFoKG9ejC",
    "outputId": "38326907-685e-4a9c-ca8b-32a7c40f1d3e"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S9AwP5lQvUN"
   },
   "source": [
    "#### 2. **Beam Search**\n",
    "\n",
    "Now, let's try using the `beam_search` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7CH6KN8Izp9",
    "outputId": "adef4782-3278-4994-9520-43e23ea047a6"
   },
   "outputs": [],
   "source": [
    "config.n=8\n",
    "# beam search specific\n",
    "config.sort_completed=True\n",
    "config.filter_duplicates=True\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='beam_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "Hw6tQD_dMwXZ",
    "outputId": "0f66c7ed-2071-45a4-e562-3967deb0bc9d"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxBBUd7HQzhd"
   },
   "source": [
    "#### 3. **Diverse Verifier Tree Search (DVTS)**\n",
    "\n",
    "Finally, let's try the `dvts` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzXW1g-dI5wN",
    "outputId": "86979d67-7dfa-4346-9adb-c386a52af58c"
   },
   "outputs": [],
   "source": [
    "config.n=8\n",
    "# dvts specific\n",
    "config.n_beams = config.n // config.beam_width\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='dvts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "RGkG9MPXMvN0",
    "outputId": "18a333ae-7b3a-455e-df2c-bb497b1381a5"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PM9HHwBSYWk"
   },
   "source": [
    "### üôã 3.2 Testing the System with a Simple Question\n",
    "\n",
    "In this final example, we‚Äôll test the system using a straightforward question to observe how it performs in simpler cases. This allows us to verify that the system works as expected even for basic queries.\n",
    "\n",
    "Let's try the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq9vM1uRM7A8",
    "outputId": "65ef318d-2b89-4d46-b660-293195c2b8e1"
   },
   "outputs": [],
   "source": [
    "question = 'What\\'s the capital of Spain?'\n",
    "\n",
    "config.n=32\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "ysfR0nPfM-Ub",
    "outputId": "b474aeb6-6cb7-4f15-ba48-fa59022f31ef"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgdeSegeANoT"
   },
   "source": [
    "Even though we set a larger number of candidate answers (`N`), the time spent thinking remains relatively small (1.03 seconds and 544 generated tokens). This demonstrates the system‚Äôs ability to efficiently handle easier problems, spending less time on them, while leveraging its enhanced capabilities for more complex questions.\n",
    "\n",
    "üèÜ **We now have a fully operational pipeline** that leverages test-time compute, enabling the system to \"think longer\" for more complicated queries, while also maintaining fast response times for straightforward questions.\n",
    "\n",
    "This approach ensures the system can scale its thinking time based on the task's complexity, offering an efficient and responsive solution for both simple and challenging problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92znAyJ0AOPY"
   },
   "source": [
    "## 4. Continuing the Journey and Resources üßë‚ÄçüéìÔ∏è\n",
    "\n",
    "If you're eager to continue exploring, be sure to check out the original experimental [blog](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) and all the references mentioned within it. These resources will deepen your understanding of test-time compute, its benefits, and its applications in LLMs.\n",
    "\n",
    "\n",
    "Happy learning and experimenting! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmarking script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-12 17:13:07 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b33aca979947b2bcac6ce15c893bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-12 17:13:14 arg_utils.py:953] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-12 17:13:14 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 02-12 17:13:14 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51640f20fa2443f99231f3852e36a294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bfb626d37144f9ba6a2f859d554282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b395c629c8e499898f5cdd6b1933e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0d305dcbff45fca1a640fd3fd587dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 17:13:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 02-12 17:13:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cbc6a11cec4b44aa45f7be8d1f0912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa517fb38464fd5bf5dff326fc38f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eab56727bc4dffa4841e36e6382845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512c8cf8fa3445bc81141c775c2a87ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a570deafcd64ef4886e40a96ed8f6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3054b5ce9ece42d0a41fa528fdfa4a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 17:13:40 model_runner.py:1071] Loading model weights took 14.9888 GB\n",
      "INFO 02-12 17:13:40 gpu_executor.py:122] # GPU blocks: 11903, # CPU blocks: 2048\n",
      "INFO 02-12 17:13:40 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 1.45x\n",
      "INFO 02-12 17:13:42 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-12 17:13:42 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-12 17:13:53 model_runner.py:1530] Graph capturing finished in 11 secs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ddfa490fff4ae69734bca9b6dc2cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from vllm import LLM\n",
    "from sal.models.reward_models import RLHFFlow\n",
    "from sal.search import beam_search, best_of_n, dvts\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_src = \"src/\"\n",
    "\n",
    "model_path=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "prm_path=\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
    "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
    "    seed=42,                     # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "prm = RLHFFlow(prm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given question using the search-and-learn pipeline.\n",
    "\n",
    "    Args:\n",
    "    - question (str): The input question to generate an answer for.\n",
    "    - config (Config): Configuration object containing parameters for search strategy.\n",
    "    - llm (LLM): Pretrained large language model used for generating answers.\n",
    "    - prm (RLHFFlow): Process reward model used for evaluating answers.\n",
    "    - method (str): Search strategy to use. Options are 'best_of_n', 'beam_search', 'dvts'. Default is 'best_of_n'.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted output after processing the question.\n",
    "    \"\"\"\n",
    "    batch = {\"problem\": [question]}\n",
    "\n",
    "    start_time = time.time()\n",
    "    if method == 'best_of_n':\n",
    "      result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n",
    "    elif method == 'beam_search':\n",
    "      result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n",
    "    elif method == 'dvts':\n",
    "      result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    total_tokens = 0\n",
    "    for completion in result['completions']:\n",
    "        for comp in  completion:\n",
    "            output_tokens = tokenizer.encode(comp)\n",
    "            total_tokens += len(output_tokens)\n",
    "\n",
    "    print(f\"Total tokens in all completions: {total_tokens}\")\n",
    "\n",
    "    formatted_output = result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
    "    return formatted_output, elapsed_time, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16, 32, 64, 128, 256]\n",
      "4\n",
      "\n",
      "Finished in 3.84 seconds\n",
      "\n",
      "Total tokens in all completions: 934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                       | 4/40 [00:08<01:15,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 8.43 seconds\n",
      "\n",
      "Total tokens in all completions: 984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                       | 4/40 [00:05<00:46,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 5.22 seconds\n",
      "\n",
      "Total tokens in all completions: 676\n",
      "8\n",
      "\n",
      "Finished in 3.95 seconds\n",
      "\n",
      "Total tokens in all completions: 1759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                      | 5/40 [00:13<01:33,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 13.33 seconds\n",
      "\n",
      "Total tokens in all completions: 1622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                       | 4/40 [00:07<01:10,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 7.84 seconds\n",
      "\n",
      "Total tokens in all completions: 1492\n",
      "16\n",
      "\n",
      "Finished in 6.22 seconds\n",
      "\n",
      "Total tokens in all completions: 3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                 | 8/40 [00:21<01:25,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 21.29 seconds\n",
      "\n",
      "Total tokens in all completions: 2772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:   8%|‚ñà‚ñà‚ñà‚ñà‚ñã                                                         | 3/40 [00:08<01:45,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 8.52 seconds\n",
      "\n",
      "Total tokens in all completions: 2933\n",
      "32\n",
      "\n",
      "Finished in 7.25 seconds\n",
      "\n",
      "Total tokens in all completions: 8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 6/40 [00:25<02:26,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 25.84 seconds\n",
      "\n",
      "Total tokens in all completions: 8519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                      | 5/40 [00:17<02:01,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 17.34 seconds\n",
      "\n",
      "Total tokens in all completions: 5601\n",
      "64\n",
      "\n",
      "Finished in 9.62 seconds\n",
      "\n",
      "Total tokens in all completions: 15138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 39/40 [06:15<00:09,  9.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 375.09 seconds\n",
      "\n",
      "Total tokens in all completions: 15586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                      | 5/40 [00:24<02:49,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 24.25 seconds\n",
      "\n",
      "Total tokens in all completions: 11232\n",
      "128\n",
      "\n",
      "Finished in 16.04 seconds\n",
      "\n",
      "Total tokens in all completions: 29355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 6/40 [01:34<08:55, 15.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 94.46 seconds\n",
      "\n",
      "Total tokens in all completions: 28882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 6/40 [00:48<04:35,  8.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished in 48.62 seconds\n",
      "\n",
      "Total tokens in all completions: 25227\n",
      "256\n",
      "\n",
      "Finished in 28.64 seconds\n",
      "\n",
      "Total tokens in all completions: 61132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search iterations:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                          | 12/40 [04:31<10:18, 22.09s/it]"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sal.config import Config\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.append(project_src)\n",
    "\n",
    "n_values = [2**i for i in range(2, 9)]\n",
    "print(n_values)\n",
    "\n",
    "# Define CSV filename\n",
    "csv_filename = \"search_methods_results.csv\"\n",
    "\n",
    "# Define methods\n",
    "methods = [\"Best-of-n\", \"Beam search\", \"Diverse verifier tree search\"]\n",
    "\n",
    "# Define headers (Each method has its own Time (s) and Total tokens)\n",
    "headers = [\"Number of generations\"]\n",
    "for method in methods:\n",
    "    headers.append(f\"{method} Time (s)\")\n",
    "    headers.append(f\"{method} Total tokens\")\n",
    "\n",
    "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)  # Write header\n",
    "\n",
    "    for i in n_values:\n",
    "        config = Config()\n",
    "        config.n = i\n",
    "\n",
    "        row = [i]\n",
    "        print(i)\n",
    "    \n",
    "        # best of n\n",
    "        _, elapsed_time_bon, token_number_bon = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')\n",
    "        row.append(elapsed_time_bon)\n",
    "        row.append(token_number_bon)\n",
    "    \n",
    "        # beam search \n",
    "        config.sort_completed=True\n",
    "        config.filter_duplicates=True\n",
    "        _, elapsed_time_beam, token_number_beam = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='beam_search')\n",
    "        row.append(elapsed_time_beam)\n",
    "        row.append(token_number_beam)\n",
    "    \n",
    "        # dvts\n",
    "        config.n_beams = config.n // config.beam_width\n",
    "        _, elapsed_time_dvts, token_number_dvts = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='dvts')\n",
    "        row.append(elapsed_time_dvts)\n",
    "        row.append(token_number_dvts)\n",
    "\n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV file '{csv_filename}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV file\n",
    "csv_filename = \"search_methods_results.csv\"  # Update this with the actual CSV file path\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Extract relevant columns\n",
    "x = df[\"Number of generations\"]\n",
    "best_of_n_time = df[\"Best-of-n Time (s)\"]\n",
    "beam_search_time = df[\"Beam search Time (s)\"]\n",
    "diverse_tree_time = df[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, best_of_n_time, marker='o', label=\"Best-of-n\")\n",
    "plt.plot(x, beam_search_time, marker='s', label=\"Beam search\")\n",
    "plt.plot(x, diverse_tree_time, marker='^', label=\"Diverse verifier tree search\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Number of Generations\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "\n",
    "plt.suptitle(\"Elapsed Time in All Completions vs. Number of Generations\", fontsize=14)\n",
    "plt.title(\"LLM: meta-llama/Llama-3.2-3B-Instruct, PRM: RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\", fontsize=10, color='gray')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Set log scale\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Ensure x-axis ticks show as integers\n",
    "plt.xticks(x, labels=[str(int(val)) for val in x])  \n",
    "\n",
    "# Grid and formatting\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "csv_filename = \"search_methods_results.csv\"  # Update this with the actual CSV file path\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Extract relevant columns\n",
    "x = df[\"Number of generations\"]\n",
    "best_of_n_tokens = df[\"Best-of-n Total tokens\"]\n",
    "beam_search_tokens = df[\"Beam search Total tokens\"]\n",
    "diverse_tree_tokens = df[\"Diverse verifier tree search Total tokens\"]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, best_of_n_tokens, marker='o', label=\"Best-of-n\")\n",
    "plt.plot(x, beam_search_tokens, marker='s', label=\"Beam search\")\n",
    "plt.plot(x, diverse_tree_tokens, marker='^', label=\"Diverse verifier tree search\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Number of Generations\")\n",
    "plt.ylabel(\"Total Tokens\")\n",
    "plt.suptitle(\"Total Tokens in All Completions vs. Number of Generations\", fontsize=14)\n",
    "plt.title(\"LLM: meta-llama/Llama-3.2-3B-Instruct, PRM: RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\", fontsize=10, color='gray')\n",
    "plt.legend()\n",
    "\n",
    "# Set log scale for better visualization\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Ensure x-axis ticks show as integers\n",
    "plt.xticks(x, labels=[str(int(val)) for val in x])  \n",
    "\n",
    "# Grid and formatting\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Llama 1b and Llama 3b on the basis of DVTS Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "llama1b_csv = \"llama_3_2_1b.csv\"  \n",
    "llama3b_csv = \"llama_3_2_3b.csv\"  \n",
    "llama8b_csv = \"llama_3_1_8b.csv\"\n",
    "\n",
    "df_1b = pd.read_csv(llama1b_csv)\n",
    "df_3b = pd.read_csv(llama3b_csv)\n",
    "df_8b = pd.read_csv(llama8b_csv)\n",
    "\n",
    "# Extract relevant data\n",
    "x_1b = df_1b[\"Number of generations\"]\n",
    "time_1b = df_1b[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "x_3b = df_3b[\"Number of generations\"]\n",
    "time_3b = df_3b[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "x_8b = df_8b[\"Number of generations\"]\n",
    "time_8b = df_8b[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_1b, time_1b, marker='o', label=\"Llama 3.2 1b\", linestyle='-')\n",
    "plt.plot(x_3b, time_3b, marker='s', label=\"Llama 3.2 3b\", linestyle='--')\n",
    "plt.plot(x_3b, time_3b, marker='x', label=\"Llama 3.1 8b\", linestyle='---')\n",
    "\n",
    "# Titles and Labels\n",
    "plt.suptitle(\"Comparison of Diverse Verifier Tree Search Time\", fontsize=14)\n",
    "plt.title(\"Llama 3.2 1b vs. Llama 3.2 3b vs Llama 3.1 8b\", fontsize=10, color='gray')\n",
    "plt.xlabel(\"Number of Generations\")\n",
    "plt.ylabel(\"Diverse Verifier Tree Search Time (s)\")\n",
    "plt.legend()\n",
    "\n",
    "# Ensure integer values on x-axis\n",
    "plt.xticks(x_1b, labels=[str(int(val)) for val in x_1b])  \n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Llama 1b and Llama 3b on the basis of DVTS Tokens number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "llama1b_csv = \"llama_3_2_1b.csv\"  \n",
    "llama3b_csv = \"llama_3_2_3b.csv\"  \n",
    "llama8b_csv = \"llama_3_1_8b.csv\"\n",
    "\n",
    "df_1b = pd.read_csv(llama1b_csv)\n",
    "df_3b = pd.read_csv(llama3b_csv)\n",
    "df_8b = pd.read_csv(llama8b_csv)\n",
    "\n",
    "# Extract relevant data\n",
    "x_1b = df_1b[\"Number of generations\"]\n",
    "tokens_1b = df_1b[\"Diverse verifier tree search Total tokens\"].astype(int)\n",
    "\n",
    "x_3b = df_3b[\"Number of generations\"]\n",
    "tokens_3b = df_3b[\"Diverse verifier tree search Total tokens\"].astype(int)\n",
    "\n",
    "x_8b = df_8b[\"Number of generations\"]\n",
    "tokens_8b = df_8b[\"Diverse verifier tree search Total tokens\"].astype(int)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_1b, tokens_1b, marker='o', label=\"Llama 1b\", linestyle='-')\n",
    "plt.plot(x_3b, tokens_3b, marker='s', label=\"Llama 3b\", linestyle='--')\n",
    "plt.plot(x_8b, tokens_8b, marker='x', label=\"Llama 8b\", linestyle='--')\n",
    "\n",
    "# Titles and Labels\n",
    "plt.suptitle(\"Comparison of Total Tokens in Diverse Verifier Tree Search\", fontsize=14)\n",
    "plt.title(\"Llama 3.2 1b vs. Llama 3.2 3b vs Llama 3.1 8b\", fontsize=10, color='gray')\n",
    "plt.xlabel(\"Number of Generations\")\n",
    "plt.ylabel(\"Total Tokens\")\n",
    "plt.legend()\n",
    "\n",
    "# Ensure integer values on x-axis\n",
    "plt.xticks(x_1b, labels=[str(int(val)) for val in x_1b])  \n",
    "\n",
    "# Ensure integer values on y-axis\n",
    "y_ticks = sorted(set(tokens_1b.tolist() + tokens_3b.tolist()))\n",
    "plt.yticks(y_ticks, labels=[str(val) for val in y_ticks])\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
