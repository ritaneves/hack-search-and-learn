{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctusmJ6BZ6_"
      },
      "source": [
        "# Scaling Test-Time Compute for Longer Thinking in LLMs\n",
        "\n",
        "_Authored by: [Sergio Paniego](https://github.com/sergiopaniego)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmgoppItAO7B"
      },
      "source": [
        "🚨 **WARNING**: This notebook is **resource-intensive** and requires substantial computational power. If you’re running this in **Colab**, it will utilize an **A100 GPU**.\n",
        "\n",
        "---\n",
        "\n",
        "In this recipe, we'll guide you through extending the inference time for an **Instruct LLM system** using **test-time compute** to solve more challenging problems, such as **complex math problems**. This approach, inspired by [**OpenAI o1-o3 models**](https://openai.com/index/learning-to-reason-with-llms/), demonstrates that **longer reasoning time** during inference can enhance model performance.\n",
        "\n",
        "This technique builds on experiments shared in [this **blog post**](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute), which show that smaller models, like the **1B** and **3B Llama Instruct models**, can outperform much larger ones on the **MATH-500 benchmark** when given enough **\"time to think\"**. Recent research from [DeepMind](https://arxiv.org/abs/2408.03314) suggests that **test-time compute** can be scaled optimally through strategies like iterative self-refinement or using a reward model.\n",
        "\n",
        "The blog introduces a [**new repository**](https://github.com/huggingface/search-and-learn) for running these experiments. In this recipe, we'll focus on building a **small chatbot** that engages in **longer reasoning** to tackle **harder problems** using small open models.\n",
        "\n",
        "![Instruct LLM Methodology](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/methods-thumbnail.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twKCzVIg71Xa"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Let’s start by installing the [search-and-learn](https://github.com/huggingface/search-and-learn) repository! 🚀  \n",
        "This repo is designed to replicate the experimental results and is not a Python pip package. However, we can still use it to generate our system. To do so, we’ll need to install it from source with the following steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t0YDC2_7XTm8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'search-and-learn'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 324, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 324 (delta 98), reused 82 (delta 82), pack-reused 183 (from 1)\u001b[K\n",
            "Receiving objects: 100% (324/324), 868.71 KiB | 3.12 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/search-and-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kT3jH_d_XcEb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/ai-query-engine/search-and-learn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///root/ai-query-engine/search-and-learn\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: latex2sympy2==1.9.1 in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (1.9.1)\n",
            "Requirement already satisfied: pebble in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (5.1.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (0.115.8)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (0.1.9)\n",
            "Requirement already satisfied: transformers>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (1.1)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (0.9.6)\n",
            "Requirement already satisfied: vllm==0.6.3 in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (0.6.3)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (8.3.4)\n",
            "Requirement already satisfied: isort in /usr/local/lib/python3.10/dist-packages (from search-and-learn==0.1.0) (6.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.7.2 in /usr/local/lib/python3.10/dist-packages (from latex2sympy2==1.9.1->search-and-learn==0.1.0) (4.7.2)\n",
            "Requirement already satisfied: sympy>=1.4 in /usr/local/lib/python3.10/dist-packages (from latex2sympy2==1.9.1->search-and-learn==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (12.570.86)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (2.10.6)\n",
            "Requirement already satisfied: lm-format-enforcer==0.10.6 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.10.6)\n",
            "Requirement already satisfied: torchvision==0.19 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.19.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (7.0.2)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.34.0)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (6.1.1)\n",
            "Requirement already satisfied: outlines<0.1,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.0.46)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (26.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (4.6.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (5.29.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (3.11.12)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.19.0)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (2.42.1)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (3.17.0)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (1.61.1)\n",
            "Requirement already satisfied: xformers==0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.0.27.post2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (11.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.21.1)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.8.1)\n",
            "Requirement already satisfied: mistral-common[opencv]>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.2.1.1.post5)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.3->search-and-learn==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.3->search-and-learn==0.1.0) (24.2)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.1.0.106)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.1.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (2024.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.1.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.8.61)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->search-and-learn==0.1.0) (0.45.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.47.0->search-and-learn==0.1.0) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.47.0->search-and-learn==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.47.0->search-and-learn==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->search-and-learn==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->search-and-learn==0.1.0) (2.2.1)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->search-and-learn==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->search-and-learn==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (4.23.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (4.11.0.86)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (0.8.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (4.7.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: pyairports in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2.1.1)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.35.1)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (5.6.3)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (24.6.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.61.0)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (2.27.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (2024.12.14)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.4->latex2sympy2==1.9.1->search-and-learn==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (2.4.6)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (24.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (1.18.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (14.2)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (1.0.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.21.1->mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.21.1->mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (0.22.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (19.0.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.44.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2025.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (1.16.0)\n",
            "Installing collected packages: search-and-learn\n",
            "  Attempting uninstall: search-and-learn\n",
            "    Found existing installation: search-and-learn 0.1.0\n",
            "    Uninstalling search-and-learn-0.1.0:\n",
            "      Successfully uninstalled search-and-learn-0.1.0\n",
            "  Running setup.py develop for search-and-learn\n",
            "Successfully installed search-and-learn-0.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd search-and-learn\n",
        "!pip install -e '.[dev]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAQHu9T176zh"
      },
      "source": [
        "Log in to Hugging Face to access [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as it is a gated model! 🗝️  \n",
        "If you haven't previously requested access, you'll need to submit a request before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pnEaTlFYZF_H"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4348041ef961462493e090fe0f832bed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX07zCTA8MWL"
      },
      "source": [
        "## 2. Setup the Large Language Model (LLM) and the Process Reward Model (PRM) 💬\n",
        "\n",
        "As illustrated in the diagram, the system consists of an LLM that generates intermediate answers based on user input, a [PRM model](https://huggingface.co/papers/2211.14275) that evaluates and scores these answers, and a search strategy that uses the PRM feedback to guide the subsequent steps in the search process until reaching the final answer.\n",
        "\n",
        "Let’s begin by initializing each model. For the LLM, we’ll use the [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) model, and for the PRM, we’ll use the [RLHFlow/Llama3.1-8B-PRM-Deepseek-Data](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkJw0x7gDJEY"
      },
      "source": [
        "![system](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/system.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MG1MolfxmZ7M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "No module named 'vllm._version'\n",
            "  from vllm.version import __version__ as VLLM_VERSION\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING 02-12 15:41:33 arg_utils.py:953] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
            "INFO 02-12 15:41:33 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
            "INFO 02-12 15:41:33 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 02-12 15:41:34 model_runner.py:1060] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
            "INFO 02-12 15:41:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
            "INFO 02-12 15:41:35 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58b6b3681ac74dd4a6c32bfb2a958668",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-12 15:41:35 model_runner.py:1071] Loading model weights took 2.3185 GB\n",
            "INFO 02-12 15:41:36 gpu_executor.py:122] # GPU blocks: 73606, # CPU blocks: 8192\n",
            "INFO 02-12 15:41:36 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 8.99x\n",
            "INFO 02-12 15:41:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 02-12 15:41:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 02-12 15:41:46 model_runner.py:1530] Graph capturing finished in 9 secs.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c3c5e337064f0f9c348aa62b5bc1a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from vllm import LLM\n",
        "from sal.models.reward_models import RLHFFlow\n",
        "\n",
        "model_path=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "prm_path=\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
        "\n",
        "llm = LLM(\n",
        "    model=model_path,\n",
        "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
        "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
        "    seed=42,                     # Set seed for reproducibility\n",
        ")\n",
        "\n",
        "prm = RLHFFlow(prm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYtPn0_V_YRx"
      },
      "source": [
        "### 2.1 Instantiate the Question, Search Strategy, and Call the Pipeline\n",
        "\n",
        "Now that we've set up the LLM and PRM, let's proceed by defining the question, selecting a search strategy to retrieve relevant information, and calling the pipeline to process the question through the models.\n",
        "\n",
        "1. **Instantiate the Question**: In this step, we define the input question that the system will answer, considering the given context.\n",
        "\n",
        "2. **Search Strategy**: The system currently supports the following search strategies: `best_of_n`, `beam_search`, and `dvts` (see diagram). For this example, we'll use `best_of_n`, but you can easily switch to any of the other strategies based on your needs. We need to define some configuration parameters for the configuration of the search strategy. You can check the full list [here](https://github.com/huggingface/search-and-learn/blob/main/src/sal/config.py).\n",
        "\n",
        "3. **Call the Pipeline**: With the question and search strategy in place, we’ll call the inference pipeline, processing the inputs through both the LLM and PRM to generate the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSWINPerJrhm"
      },
      "source": [
        "![](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/search-strategies.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z69xD6i2L5a6"
      },
      "source": [
        "The first step is to clearly define the question that the system will answer. This ensures that we have a precise task for the model to tackle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "83puLxhzsOM0"
      },
      "outputs": [],
      "source": [
        "question_text = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
        "input_batch = {\"problem\": [question_text]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGpyzMNkAO7H"
      },
      "source": [
        "Next, we define the configuration, including parameters like the number of candidate answers `(N)`, and choose the search strategy that will be used. The search strategy dictates how we explore the potential answers. In this case, we'll use `best_of_n`.\n",
        "\n",
        "With the question and configuration in place, we use the selected search strategy to generate multiple candidate answers. These candidates are evaluated based on their relevance and quality and the final answer is returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C6s6GS16QZLV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from sal.config import Config\n",
        "from sal.search import beam_search, best_of_n, dvts\n",
        "\n",
        "config = Config()\n",
        "config.n=32 # Number of answers to generate during the search\n",
        "\n",
        "search_result = best_of_n(x=input_batch, config=config, llm=llm, prm=prm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsLHD_6C_15p"
      },
      "source": [
        "### 2.2 Display the Final Result\n",
        "\n",
        "Once the pipeline has processed the question through the LLM and PRM, we can display the final result. This result will be the model's output after considering the intermediate answers and scoring them using the PRM.\n",
        "\n",
        "Here's how to display the final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "v8medbURbgdI",
        "outputId": "3620f3e6-a25d-4bec-f41c-c4f03a6ed770"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'## Step 1: Understand the relationship between rectangular and polar coordinates\\nThe relationship between rectangular coordinates $(x, y)$ and polar coordinates $(r, \\\\theta)$ is given by $x = r \\\\cos \\\\theta$ and $y = r \\\\sin \\\\theta$. Here, we need to find $r$ and $\\\\theta$ for the point $(0, 3)$.\\n\\n## Step 2: Find the radius $r$\\nWe know that $y = r \\\\sin \\\\theta$ and $y = 3$. So, we can set $3 = r \\\\sin \\\\theta$. To find $r$, we need to find the value of $\\\\theta$.\\n\\n## Step 3: Find the value of $\\\\theta$\\nWe know that $\\\\cos 0 = 1$ and $\\\\cos \\\\frac{\\\\pi}{2} = 0$. Since $y = 3$ is positive, $\\\\sin \\\\theta$ must be positive. This means $\\\\theta$ must be in the first or second quadrant. For this point, it is more likely that $\\\\theta$ is $\\\\frac{\\\\pi}{2}$ because $\\\\sin \\\\frac{\\\\pi}{2} = 1$.\\n\\n## Step 4: Calculate the radius $r$\\nNow that we know $\\\\theta = \\\\frac{\\\\pi}{2}$, we can calculate $r$. Since $r = y = 3$ and $\\\\sin \\\\theta = 1$, we have $r = 3 \\\\sin \\\\frac{\\\\pi}{2} = 3$.\\n\\n## Step 5: Write the polar coordinates\\nNow that we have found $r$ and $\\\\theta$, the polar coordinates of the point $(0, 3)$ are $(r, \\\\theta) = \\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\n## Step 6: Provide the final answer\\nTherefore, the polar coordinates of the point $(0, 3)$ are $\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)}$'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_result['pred'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8hIu05AO7J"
      },
      "source": [
        "The model’s output might include special tokens, such as `<|start_header_id|>` or `<|end_header_id|>`. To make the answer more readable, we can safely remove them before displaying it to the end user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "flbIu6-rDapM",
        "outputId": "fcb197d5-0f21-4953-8a21-869c92a1f957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'## Step 1: Understand the relationship between rectangular and polar coordinates\\nThe relationship between rectangular coordinates $(x, y)$ and polar coordinates $(r, \\\\theta)$ is given by $x = r \\\\cos \\\\theta$ and $y = r \\\\sin \\\\theta$. Here, we need to find $r$ and $\\\\theta$ for the point $(0, 3)$.\\n\\n## Step 2: Find the radius $r$\\nWe know that $y = r \\\\sin \\\\theta$ and $y = 3$. So, we can set $3 = r \\\\sin \\\\theta$. To find $r$, we need to find the value of $\\\\theta$.\\n\\n## Step 3: Find the value of $\\\\theta$\\nWe know that $\\\\cos 0 = 1$ and $\\\\cos \\\\frac{\\\\pi}{2} = 0$. Since $y = 3$ is positive, $\\\\sin \\\\theta$ must be positive. This means $\\\\theta$ must be in the first or second quadrant. For this point, it is more likely that $\\\\theta$ is $\\\\frac{\\\\pi}{2}$ because $\\\\sin \\\\frac{\\\\pi}{2} = 1$.\\n\\n## Step 4: Calculate the radius $r$\\nNow that we know $\\\\theta = \\\\frac{\\\\pi}{2}$, we can calculate $r$. Since $r = y = 3$ and $\\\\sin \\\\theta = 1$, we have $r = 3 \\\\sin \\\\frac{\\\\pi}{2} = 3$.\\n\\n## Step 5: Write the polar coordinates\\nNow that we have found $r$ and $\\\\theta$, the polar coordinates of the point $(0, 3)$ are $(r, \\\\theta) = \\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\n## Step 6: Provide the final answer\\nTherefore, the polar coordinates of the point $(0, 3)$ are $\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)}$'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_output = search_result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
        "formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZuLZNirAO7J"
      },
      "source": [
        "After removing any special tokens, we can display the final answer to the user. Since the answer is based on markdown, it can be rendered properly by displaying it as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "P4En0qJRD0cl",
        "outputId": "56400fea-e304-4f16-d255-909f42f636e0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: Understand the relationship between rectangular and polar coordinates\n",
              "The relationship between rectangular coordinates $(x, y)$ and polar coordinates $(r, \\theta)$ is given by $x = r \\cos \\theta$ and $y = r \\sin \\theta$. Here, we need to find $r$ and $\\theta$ for the point $(0, 3)$.\n",
              "\n",
              "## Step 2: Find the radius $r$\n",
              "We know that $y = r \\sin \\theta$ and $y = 3$. So, we can set $3 = r \\sin \\theta$. To find $r$, we need to find the value of $\\theta$.\n",
              "\n",
              "## Step 3: Find the value of $\\theta$\n",
              "We know that $\\cos 0 = 1$ and $\\cos \\frac{\\pi}{2} = 0$. Since $y = 3$ is positive, $\\sin \\theta$ must be positive. This means $\\theta$ must be in the first or second quadrant. For this point, it is more likely that $\\theta$ is $\\frac{\\pi}{2}$ because $\\sin \\frac{\\pi}{2} = 1$.\n",
              "\n",
              "## Step 4: Calculate the radius $r$\n",
              "Now that we know $\\theta = \\frac{\\pi}{2}$, we can calculate $r$. Since $r = y = 3$ and $\\sin \\theta = 1$, we have $r = 3 \\sin \\frac{\\pi}{2} = 3$.\n",
              "\n",
              "## Step 5: Write the polar coordinates\n",
              "Now that we have found $r$ and $\\theta$, the polar coordinates of the point $(0, 3)$ are $(r, \\theta) = \\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "## Step 6: Provide the final answer\n",
              "Therefore, the polar coordinates of the point $(0, 3)$ are $\\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uCpYzAw_4o9"
      },
      "source": [
        "## 3. Assembling It All! 🧑‍🏭️\n",
        "\n",
        "Now, let's create a method that encapsulates the entire pipeline. This will allow us to easily reuse the process in future applications, making it efficient and modular.\n",
        "\n",
        "By combining the LLM, PRM, search strategy, and result display, we can simplify the workflow and ensure that it’s reusable for other tasks or questions.\n",
        "\n",
        "We simplify the workflow, ensuring that it’s reusable for different tasks or questions. Additionally, we’ll track the time spent on each method so that we can **understand the practical implications** of using each strategy and configuration.\n",
        "\n",
        "Here’s how we can structure the method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YpswbcVi37KR"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n",
        "    \"\"\"\n",
        "    Generate an answer for a given question using the search-and-learn pipeline.\n",
        "\n",
        "    Args:\n",
        "    - question (str): The input question to generate an answer for.\n",
        "    - config (Config): Configuration object containing parameters for search strategy.\n",
        "    - llm (LLM): Pretrained large language model used for generating answers.\n",
        "    - prm (RLHFFlow): Process reward model used for evaluating answers.\n",
        "    - method (str): Search strategy to use. Options are 'best_of_n', 'beam_search', 'dvts'. Default is 'best_of_n'.\n",
        "\n",
        "    Returns:\n",
        "    - str: The formatted output after processing the question.\n",
        "    \"\"\"\n",
        "    batch = {\"problem\": [question]}\n",
        "\n",
        "    start_time = time.time()\n",
        "    if method == 'best_of_n':\n",
        "      result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n",
        "    elif method == 'beam_search':\n",
        "      result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n",
        "    elif method == 'dvts':\n",
        "      result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n",
        "\n",
        "    tokenizer = llm.get_tokenizer()\n",
        "    total_tokens = 0\n",
        "    for completion in result['completions']:\n",
        "        for comp in  completion:\n",
        "            output_tokens = tokenizer.encode(comp)\n",
        "            total_tokens += len(output_tokens)\n",
        "\n",
        "    print(f\"Total tokens in all completions: {total_tokens}\")\n",
        "\n",
        "    formatted_output = result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
        "    return formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWbOqkiKPVd2"
      },
      "source": [
        "### ⏳  3.1 Comparing Thinking Time for Each Strategy\n",
        "\n",
        "Let’s compare the **thinking time** of three methods: `best_of_n`, `beam_search`, and `dvts`. Each method is evaluated using the same number of answers during the search process, measuring the time spent thinking in seconds and the number of generated tokens.\n",
        "\n",
        "In the results below, the `best_of_n` method shows the least thinking time, while the `dvts` method takes the most time. However, `best_of_n` generates more tokens due to its simpler search strategy.\n",
        "\n",
        "| **Method**      | **Number of Answers During Search** | **Thinking Time (Seconds)** | **Generated Tokens** |\n",
        "|------------------|-------------------------------------|-----------------------------|-----------------------|\n",
        "| **best_of_n**    | 8                                   | 3.54                        | 3087                  |\n",
        "| **beam_search**  | 8                                   | 10.06                       | 2049                  |\n",
        "| **dvts**         | 8                                   | 8.46                        | 2544                  |\n",
        "\n",
        "This comparison illustrates the trade-offs between the strategies, balancing time spent thinking and the complexity of the search process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ROJwROGX8q-"
      },
      "source": [
        "#### 1. **Best of n**\n",
        "\n",
        "We’ll begin by using the `best_of_n` strategy. Here’s how to track the thinking time for this method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_fWKy5CCTLV",
        "outputId": "8d77eea3-b23e-4eba-cfe3-5935fae1405d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 3.54 seconds\n",
            "\n",
            "Total tokens in all completions: 3124\n"
          ]
        }
      ],
      "source": [
        "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
        "\n",
        "config.n=8\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "uzKfFoKG9ejC",
        "outputId": "38326907-685e-4a9c-ca8b-32a7c40f1d3e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: Recall the conversion formulas from rectangular to polar coordinates\n",
              "The conversion formulas are $r = \\sqrt{x^2 + y^2}$ for the radial coordinate and $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$ for the angular coordinate.\n",
              "\n",
              "## Step 2: Substitute the given rectangular coordinates into the formulas\n",
              "Given the point $(0, 3)$, we substitute $x = 0$ and $y = 3$ into the formulas.\n",
              "\n",
              "## Step 3: Calculate the radial coordinate\n",
              "$r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3$\n",
              "\n",
              "## Step 4: Calculate the angular coordinate\n",
              "$\\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right) = \\tan^{-1}(\\infty) = \\frac{\\pi}{2}$\n",
              "\n",
              "## Step 5: Combine the results\n",
              "The polar coordinates of the point $(0, 3)$ are $\\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9AwP5lQvUN"
      },
      "source": [
        "#### 2. **Beam Search**\n",
        "\n",
        "Now, let's try using the `beam_search` strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7CH6KN8Izp9",
        "outputId": "adef4782-3278-4994-9520-43e23ea047a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam search iterations:  20%|██        | 8/40 [00:10<00:40,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 10.06 seconds\n",
            "\n",
            "Total tokens in all completions: 2049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.n=8\n",
        "# beam search specific\n",
        "config.sort_completed=True\n",
        "config.filter_duplicates=True\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='beam_search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Hw6tQD_dMwXZ",
        "outputId": "0f66c7ed-2071-45a4-e562-3967deb0bc9d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: To convert the point (0,3) from rectangular coordinates to polar coordinates, we need to find the radius (r) and the angle (heta).\n",
              "\n",
              "The formula to convert from rectangular coordinates (x, y) to polar coordinates (r, heta) is given by:\n",
              "r = sqrt(x^2 + y^2)\n",
              "heta = atan2(y, x)\n",
              "\n",
              "## Step 2: Plug in the values (0,3) into the formula to find the radius (r).\n",
              "\n",
              "r = sqrt(0^2 + 3^2)\n",
              "r = sqrt(0 + 9)\n",
              "r = sqrt(9)\n",
              "r = 3\n",
              "\n",
              "## Step 3: Plug in the values (0,3) into the formula to find the angle (heta).\n",
              "\n",
              "heta = atan2(3, 0)\n",
              "Since the point (0,3) is in the first quadrant and lies on the positive y-axis, heta = pi/2 (or 90 degrees).\n",
              "\n",
              "## Step 4: Combine r and heta to get the polar coordinates.\n",
              "\n",
              "The polar coordinates are (3, pi/2).\n",
              "\n",
              "The final answer is: $\\boxed{(3, \\frac{\\pi}{2})}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxBBUd7HQzhd"
      },
      "source": [
        "#### 3. **Diverse Verifier Tree Search (DVTS)**\n",
        "\n",
        "Finally, let's try the `dvts` strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzXW1g-dI5wN",
        "outputId": "86979d67-7dfa-4346-9adb-c386a52af58c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam search iterations:  22%|██▎       | 9/40 [00:08<00:29,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 8.46 seconds\n",
            "\n",
            "Total tokens in all completions: 2544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.n=8\n",
        "# dvts specific\n",
        "config.n_beams = config.n // config.beam_width\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='dvts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "RGkG9MPXMvN0",
        "outputId": "18a333ae-7b3a-455e-df2c-bb497b1381a5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: To convert the point (0,3) from rectangular coordinates to polar coordinates, we need to find the radius r and the angle theta.\n",
              "\n",
              "The radius r can be calculated using the formula $r = \\sqrt{x^2 + y^2}$, where x is the x-coordinate and y is the y-coordinate.\n",
              "\n",
              "## Step 2: Substitute the values of x and y into the formula to find the radius r.\n",
              "\n",
              "$r = \\sqrt{0^2 + 3^2}$\n",
              "$r = \\sqrt{9}$\n",
              "$r = 3$\n",
              "\n",
              "## Step 3: Now that we have the radius r, we can find the angle theta using the formula $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$.\n",
              "\n",
              "Since x = 0 and y = 3, the angle theta is 90 degrees or $\\frac{\\pi}{2}$ radians.\n",
              "\n",
              "## Step 4: Now that we have the radius r and the angle theta, we can write the polar coordinates as (r, theta).\n",
              "\n",
              "Therefore, the polar coordinates for the point (0, 3) are $\\left(3, \\frac{\\pi}{2}\\right).$\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PM9HHwBSYWk"
      },
      "source": [
        "### 🙋 3.2 Testing the System with a Simple Question\n",
        "\n",
        "In this final example, we’ll test the system using a straightforward question to observe how it performs in simpler cases. This allows us to verify that the system works as expected even for basic queries.\n",
        "\n",
        "Let's try the following question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9vM1uRM7A8",
        "outputId": "65ef318d-2b89-4d46-b660-293195c2b8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 0.13 seconds\n",
            "\n",
            "Total tokens in all completions: 59\n"
          ]
        }
      ],
      "source": [
        "question = 'What\\'s the capital of Spain?'\n",
        "\n",
        "config.n=8\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "ysfR0nPfM-Ub",
        "outputId": "b474aeb6-6cb7-4f15-ba48-fa59022f31ef"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The capital of Spain is Madrid."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgdeSegeANoT"
      },
      "source": [
        "Even though we set a larger number of candidate answers (`N`), the time spent thinking remains relatively small (1.03 seconds and 544 generated tokens). This demonstrates the system’s ability to efficiently handle easier problems, spending less time on them, while leveraging its enhanced capabilities for more complex questions.\n",
        "\n",
        "🏆 **We now have a fully operational pipeline** that leverages test-time compute, enabling the system to \"think longer\" for more complicated queries, while also maintaining fast response times for straightforward questions.\n",
        "\n",
        "This approach ensures the system can scale its thinking time based on the task's complexity, offering an efficient and responsive solution for both simple and challenging problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Generating Evaluation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prep import preparing_input_dataset, preparing_output_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name = \"allenai/math_qa\"\n",
        "split = \"test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "preparing_input_dataset() takes from 0 to 2 positional arguments but 3 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_entries, all_options \u001b[38;5;241m=\u001b[39m \u001b[43mpreparing_input_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: preparing_input_dataset() takes from 0 to 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "all_entries, all_options = preparing_input_dataset(dataset_name, split, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 2.21 seconds\n",
            "\n",
            "Total tokens in all completions: 2736\n",
            "## Step 1: Let's assume the cost price of the article is $100.\n",
            "\n",
            "## Step 2: Since there was a 31.1% profit after a 5% discount, we can set up an equation to represent the situation: Selling Price = Cost Price * (1 + Profit Percentage).\n",
            "\n",
            "## Step 3: The profit earned on the article is $100 * 31.1% = $31.1, so the selling price after the discount would be $100 + $31.1 = $131.1.\n",
            "\n",
            "## Step 4: Now we can calculate the cost price before the discount: Cost Price = Selling Price / (1 - Discount Percentage) = $131.1 / (1 - 0.05) = $131.1 / 0.95 = $138.49.\n",
            "\n",
            "## Step 5: To find the percentage of profit earned before the discount, we can calculate the profit: Profit = Cost Price * (1 - Cost Percentage) = $138.49 * (1 - 0.05) = $138.49 * 0.95 = $131.295.\n",
            "\n",
            "## Step 6: Now we can calculate the percentage of profit earned before the discount: Percentage Profit = (Profit / Cost Price) * 100 = ($131.295 / $138.49) * 100 = 0.94 * 100 = 94%.\n",
            "\n",
            "The final answer is: $\\boxed{94}$\n",
            "\n",
            "Finished in 2.10 seconds\n",
            "\n",
            "Total tokens in all completions: 2166\n",
            "To find the difference between simple and compound interest, we need to calculate both types of interest separately.\n",
            "\n",
            "Simple Interest (SI) = (Principal x Rate x Time) / 100\n",
            "= (100 x 14 x 4) / 100\n",
            "= 560\n",
            "\n",
            "Compound Interest (CI) = P x ((1 + R/100)^n - 1)\n",
            "= 1000 x ((1 + 14/100)^4 - 1)\n",
            "= 1000 x ((1.14)^4 - 1)\n",
            "= 1000 x (1.57354 - 1)\n",
            "= 1000 x 0.57354\n",
            "= 573.54\n",
            "\n",
            "Difference = CI - SI\n",
            "= 573.54 - 560\n",
            "= 13.54\n",
            "\n",
            "Therefore, the difference between simple and compound interest at 14% per annum on a sum of Rs. 1000 after 4 years is Rs. 13.54.\n",
            "\n",
            "Finished in 2.06 seconds\n",
            "\n",
            "Total tokens in all completions: 1752\n",
            "## Step 1: Calculate the total number of passengers\n",
            "To find the total number of passengers, we need to know the total number of people who are traveling between the two cities. \n",
            "\n",
            "## Step 2: Calculate the total number of passengers\n",
            "Since there are 28 stations and people have to travel between any two stations, we have to use the combination of stations to calculate the total number of passengers. The combination of n stations taken r at a time is calculated as nCr = n! / [(n-r)!r!]. However, in this problem, we are assuming every passenger is unique and traveled to another unique station, hence, we need to use the combination formula for this specific case.\n",
            "\n",
            "## Step 3: Calculate the total number of passengers\n",
            "The total number of passengers is given by the combination of 28 stations taken 2 at a time: C(28, 2) = 28! / [(28-2)!2!] = 28 * 27 / 2 = 378.\n",
            "\n",
            "## Step 4: Calculate the number of second-class tickets needed\n",
            "Since each passenger can travel to any other station, we need to find the number of unique combinations of 2 tickets needed. This is equal to the number of combinations of 28 stations taken 2 at a time.\n",
            "\n",
            "## Step 5: Calculate the number of second-class tickets\n",
            "The number of second-class tickets needed is therefore C(28, 2) = 378.\n",
            "\n",
            "The final answer is: $\\boxed{378}$\n",
            "\n",
            "Finished in 1.71 seconds\n",
            "\n",
            "Total tokens in all completions: 1858\n",
            "## Step 1: Calculate the population increase rate as a decimal.\n",
            "The population increase rate is 20% per annum. To find the decimal representation, divide by 100: 20 / 100 = 0.2.\n",
            "\n",
            "## Step 2: Use the formula for population increase to find the population before 2 years.\n",
            "The formula for population increase is A = P(1 + r)^t, where A is the amount after t years, P is the principal amount (initial population), r is the rate of increase, and t is the time period. In this case, we are solving for P before 2 years.\n",
            "\n",
            "## Step 3: Substitute the given values into the formula.\n",
            "Given A = 3888, r = 0.2, and t = 2, substitute these values into the formula: 3888 = P(1 + 0.2)^2.\n",
            "\n",
            "## Step 4: Solve for P.\n",
            "First, calculate (1 + 0.2) = 1.2. Then, raise 1.2 to the power of 2: (1.2)^2 = 1.44. Now, set up the equation: 3888 = P * 1.44.\n",
            "\n",
            "## Step 5: Find the value of P.\n",
            "Divide both sides of the equation by 1.44 to isolate P: P = 3888 / 1.44.\n",
            "\n",
            "## Step 6: Calculate P.\n",
            "Perform the division: P = 2720.\n",
            "\n",
            "The final answer is: $\\boxed{2720}$\n",
            "\n",
            "Finished in 1.03 seconds\n",
            "\n",
            "Total tokens in all completions: 1043\n",
            "## Step 1: Understanding the problem\n",
            "The problem is asking for the triplicate ratio of 1 to 9, which means we need to cube both numbers.\n",
            "\n",
            "## Step 2: Calculating the cube\n",
            "To find the cube of 1, we simply multiply it by itself: 1 * 1 = 1.\n",
            "To find the cube of 9, we multiply it by itself three times: 9 * 9 * 9 = 729.\n",
            "\n",
            "## Step 3: Finding the triplicate ratio\n",
            "Now that we have the cubes of both numbers, we can find the triplicate ratio by dividing the cube of 9 by the cube of 1: 729 / 1 = 729.\n",
            "\n",
            "The final answer is: $\\boxed{729}$\n",
            "\n",
            "Finished in 1.64 seconds\n",
            "\n",
            "Total tokens in all completions: 1992\n",
            "## Step 1: Identify the range of numbers\n",
            "The given range is -26 < s < 24.\n",
            "\n",
            "## Step 2: Calculate the sum of an arithmetic series\n",
            "To find the sum, we need to know the number of terms. Since the difference between consecutive numbers is constant (-26 + 24 = -2), we can find the number of terms by dividing the difference by the common difference and adding 1. number_of_terms = (24 - (-26)) / 2 = 50.\n",
            "\n",
            "## Step 3: Calculate the sum of the arithmetic series\n",
            "The sum of an arithmetic series is given by the formula: sum = (number_of_terms / 2) * (first_number + last_number). The first number is -26 and the last number is 24.\n",
            "\n",
            "## Step 4: Substitute values into the formula\n",
            "sum = (50 / 2) * (-26 + 24) = 25 * -2 = -50.\n",
            "\n",
            "The final answer is: $\\boxed{-50}$\n",
            "\n",
            "Finished in 7.28 seconds\n",
            "\n",
            "Total tokens in all completions: 5342\n",
            "## Step 1: Calculate the volume of oil pumped to the truck\n",
            "The volume of oil pumped to the truck can be calculated using the formula for the volume of a cylinder, which is V = πr^2h, where r is the radius of the truck's tank and h is its height. So, V_truck = π * 6^2 * 10.\n",
            "\n",
            "## Step 2: Calculate the base area of the stationary tank\n",
            "The base area of the stationary tank can be calculated using the formula for the area of a circle, which is A = πr^2. So, A_stationary = π * 100^2.\n",
            "\n",
            "## Step 3: Calculate the volume of oil pumped to the truck in terms of the difference between the volumes of the two tanks\n",
            "Let x be the height by which the oil level drop in the stationary tank. Then the volume of oil in the truck is π * 6^2 * 10 = 360π, and the volume of oil in the stationary tank is π * 100^2 * x. The difference in volume is 360π - π * 100^2 * x.\n",
            "\n",
            "## Step 4: Set up an equation based on the volume of oil pumped to the truck\n",
            "Since the truck's tank is completely filled, the volume of oil pumped to the truck is equal to the volume of oil in the stationary tank. Setting up the equation: 360π = π * 100^2 * x.\n",
            "\n",
            "## Step 5: Solve the equation for x\n",
            "Divide both sides of the equation by π * 100^2: 360π / (π * 100^2) = x.\n",
            "Cancel π on both sides: 360 / (100^2) = x.\n",
            "Calculate x: 360 / 10000 = x.\n",
            "x = 0.036.\n",
            "\n",
            "## Step 6: Calculate the distance the oil level dropped in the stationary tank\n",
            "Since the truck's tank has a radius of 6 feet and its height is 10 feet, the volume of oil pumped to the truck is also equal to the volume of oil in the truck tank. Therefore, 360π = π * 6^2 * 10.\n",
            "Cancel π on both sides: 360 = 6^2 * 10.\n",
            "Calculate 6^2: 6^2 = 36.\n",
            "Calculate the volume of oil in the truck tank: 360 = 36 * 10.\n",
            "Calculate the x value: 360 / 360 = 1.\n",
            "Since the initial oil level was at the top of the tank, the drop is equal to the x value we calculated in the previous step. Therefore, the distance the oil level dropped in the stationary tank is 1 foot.\n",
            "\n",
            "The final answer is: $\\boxed{1}$\n",
            "\n",
            "Finished in 6.57 seconds\n",
            "\n",
            "Total tokens in all completions: 4804\n",
            "## Step 1: Let's define the variables.\n",
            "Let L be the number of larger cans and S be the number of smaller cans.\n",
            "\n",
            "## Step 2: Set up the equation based on the problem.\n",
            "Since the restaurant must order 20 more of the smaller cans than the larger cans, we can set up the equation: S = L + 20.\n",
            "\n",
            "## Step 3: The total volume of chili paste is the same in both cans, so we can set up another equation: 35L = 25S.\n",
            "\n",
            "## Step 4: Substitute the expression for S from step 2 into the equation from step 3.\n",
            "35L = 25(L + 20).\n",
            "\n",
            "## Step 5: Simplify the equation.\n",
            "35L = 25L + 500.\n",
            "\n",
            "## Step 6: Subtract 25L from both sides to isolate L.\n",
            "10L = 500.\n",
            "\n",
            "## Step 7: Divide both sides by 10 to solve for L.\n",
            "L = 50.\n",
            "\n",
            "## Step 8: Substitute the value of L into the equation S = L + 20 to find the number of smaller cans.\n",
            "S = 50 + 20 = 70.\n",
            "\n",
            "The final answer is: $\\boxed{70}$\n",
            "\n",
            "Finished in 1.96 seconds\n",
            "\n",
            "Total tokens in all completions: 1972\n",
            "## Step 1: Understand the problem\n",
            "We need to find the greatest possible value of n such that 101n^2 is less than or equal to 10000.\n",
            "\n",
            "## Step 2: Write down the inequality\n",
            "The given condition can be written as: 101n^2 ≤ 10000.\n",
            "\n",
            "## Step 3: Solve for n^2\n",
            "Divide both sides of the inequality by 101 to isolate n^2: n^2 ≤ 10000 / 101.\n",
            "\n",
            "## Step 4: Calculate the right-hand side\n",
            "Calculate the value of 10000 / 101: n^2 ≤ 99.0408163265306122.\n",
            "\n",
            "## Step 5: Take the square root of both sides\n",
            "Take the square root of both sides of the inequality to solve for n: n ≤ √99.0408163265306122.\n",
            "\n",
            "## Step 6: Calculate the square root\n",
            "Calculate the value of √99.0408163265306122: n ≤ 9.99.\n",
            "\n",
            "## Step 7: Determine the greatest integer value of n\n",
            "Since n is an integer, the greatest possible value of n is 9.\n",
            "\n",
            "The final answer is: $\\boxed{9}$\n",
            "\n",
            "Finished in 1.44 seconds\n",
            "\n",
            "Total tokens in all completions: 1699\n",
            "## Step 1: Calculate the total number of man-days required for the job.\n",
            "To find the total number of man-days required, we multiply the number of people by the number of days: 10 people * 4 days = 40 man-days.\n",
            "\n",
            "## Step 2: Calculate the number of man-days per person when 5 people work.\n",
            "Now, we divide the total number of man-days by the number of people: 40 man-days / 10 people = 4 man-days per person.\n",
            "\n",
            "## Step 3: Calculate the number of days required for 5 people to complete the job.\n",
            "To find the number of days required for 5 people to complete the job, we divide the total number of man-days by the number of people: 40 man-days / 5 people = 8 days.\n",
            "\n",
            "The final answer is: $\\boxed{8}$\n",
            "\n",
            "Finished in 1.77 seconds\n",
            "\n",
            "Total tokens in all completions: 1866\n",
            "## Step 1: Calculate the population decrease for the first year\n",
            "To find the population after 3 years, we first need to calculate the decrease in population for the first year. The town's population decreases annually at a rate of 20%.\n",
            "\n",
            "## Step 2: Calculate the population after the first year\n",
            "Population after 1 year = Initial population * (1 - decrease rate)\n",
            "Population after 1 year = 8000 * (1 - 0.20)\n",
            "Population after 1 year = 8000 * 0.80\n",
            "Population after 1 year = 6400\n",
            "\n",
            "## Step 3: Calculate the population decrease for the second year\n",
            "Now, we need to calculate the decrease in population for the second year.\n",
            "\n",
            "## Step 4: Calculate the population after the second year\n",
            "Population after 2 years = Population after 1 year * (1 - decrease rate)\n",
            "Population after 2 years = 6400 * (1 - 0.20)\n",
            "Population after 2 years = 6400 * 0.80\n",
            "Population after 2 years = 5120\n",
            "\n",
            "## Step 5: Calculate the population decrease for the third year\n",
            "Similarly, we need to calculate the decrease in population for the third year.\n",
            "\n",
            "## Step 6: Calculate the population after the third year\n",
            "Population after 3 years = Population after 2 years * (1 - decrease rate)\n",
            "Population after 3 years = 5120 * (1 - 0.20)\n",
            "Population after 3 years = 5120 * 0.80\n",
            "Population after 3 years = 4096\n",
            "\n",
            "The final answer is: $\\boxed{4096}$\n",
            "\n",
            "Finished in 3.31 seconds\n",
            "\n",
            "Total tokens in all completions: 3281\n",
            "## Step 1: Let's assume the cost price (CP) of the article is x.\n",
            "\n",
            "## Step 2: The percentage profit earned by selling the article for Rs. 1920 is (1920 - x) / x * 100.\n",
            "\n",
            "## Step 3: The percentage loss incurred by selling the same article for Rs. 1280 is (x - 1280) / x * 100.\n",
            "\n",
            "## Step 4: According to the problem, these two percentages are equal, so we can set up the equation (1920 - x) / x * 100 = (x - 1280) / x * 100.\n",
            "\n",
            "## Step 5: Simplifying the equation, we get 1920 - x = x - 1280.\n",
            "\n",
            "## Step 6: Combining like terms, we get 1920 + 1280 = 2x.\n",
            "\n",
            "## Step 7: Simplifying further, we get 3200 = 2x.\n",
            "\n",
            "## Step 8: Dividing both sides by 2, we get x = 1600.\n",
            "\n",
            "## Step 9: Now that we know the cost price, we can calculate the selling price to make a 40% profit. To do this, we can use the formula: Selling Price = Cost Price + (Cost Price * 40 / 100).\n",
            "\n",
            "## Step 10: Plugging in the value of CP, we get Selling Price = 1600 + (1600 * 40 / 100).\n",
            "\n",
            "## Step 11: Simplifying the expression, we get Selling Price = 1600 + 640.\n",
            "\n",
            "## Step 12: Combining like terms, we get Selling Price = 2240.\n",
            "\n",
            "## Step 13: Therefore, the article should be sold for Rs. 2240 to make a 40% profit.\n",
            "\n",
            "The final answer is: $\\boxed{2240}$\n",
            "\n",
            "Finished in 1.52 seconds\n",
            "\n",
            "Total tokens in all completions: 1640\n",
            "## Step 1: Calculate the rate of one machine\n",
            "To find the rate at which one machine produces bottles, we divide the total number of bottles produced per minute by the number of machines. So, 360 bottles per minute / 6 machines = 60 bottles per minute per machine.\n",
            "\n",
            "## Step 2: Calculate the rate of 10 machines\n",
            "Since we know one machine can produce 60 bottles per minute, we multiply this rate by 10 to find the rate of 10 machines. Thus, 60 bottles per minute * 10 machines = 600 bottles per minute.\n",
            "\n",
            "## Step 3: Calculate the total number of bottles produced in 4 minutes\n",
            "To find the total number of bottles produced in 4 minutes, we multiply the rate of 10 machines by the number of minutes. Therefore, 600 bottles per minute * 4 minutes = 2400 bottles.\n",
            "\n",
            "The final answer is: $\\boxed{2400}$\n",
            "\n",
            "Finished in 1.86 seconds\n",
            "\n",
            "Total tokens in all completions: 1087\n",
            "## Step 1: Calculate the number of zeroes needed in numbers ending in 0.\n",
            "To find the number of zeroes needed for numbers ending in 0, we calculate the number of factors of 10, which is 2 * 5.\n",
            "\n",
            "## Step 2: Calculate the number of zeroes needed in numbers ending in 5.\n",
            "To find the number of zeroes needed for numbers ending in 5, we calculate the number of factors of 5, which is 5.\n",
            "\n",
            "## Step 3: Calculate the number of zeroes needed in numbers ending in 25.\n",
            "To find the number of zeroes needed for numbers ending in 25, we calculate the number of factors of 5, which is 2.\n",
            "\n",
            "## Step 4: Calculate the number of zeroes needed in numbers ending in 125.\n",
            "To find the number of zeroes needed for numbers ending in 125, we calculate the number of factors of 5, which is 3.\n",
            "\n",
            "## Step 5: Calculate the number of zeroes needed in numbers ending in 625.\n",
            "To find the number of zeroes needed for numbers ending in 625, we calculate the number of factors of 5, which is 4.\n",
            "\n",
            "\n",
            "## Step 6: Sum the number of zeroes needed for numbers ending in 0, 5, 25, 125, and 625 to find the total number of zeroes needed.\n",
            "Total number of zeroes = (2 * 1000) + (5 * 100) + (2 * 25) + (3 * 5) + (4 * 1)\n",
            "\n",
            "## Step 7: Calculate the sum.\n",
            "Total number of zeroes = 2000 + 500 + 50 + 15 + 4\n",
            "\n",
            "## Step 8: Sum the calculated values.\n",
            "Total number of zeroes = 2629\n",
            "\n",
            "\n",
            "The final answer is: $\\boxed{2629}$\n",
            "\n",
            "Finished in 1.43 seconds\n",
            "\n",
            "Total tokens in all completions: 1608\n",
            "## Step 1: Calculate the total cost of the shares\n",
            "To find the total cost of the shares, we need to calculate the discounted price of one share. The discounted price of one share is 50 - 5 = 45. Since the man bought 20 shares, the total cost of the shares is 20 * 45 = 900.\n",
            "\n",
            "## Step 2: Calculate the dividend per share\n",
            "To find the dividend per share, we need to multiply the total cost of the shares by the rate of dividend. The dividend per share is 900 * 13 = 11700.\n",
            "\n",
            "## Step 3: Calculate the rate of interest\n",
            "To find the rate of interest, we need to divide the dividend per share by the total cost of the shares. The rate of interest is 11700 / 900 = 13.\n",
            "\n",
            "The final answer is: $\\boxed{13}$\n",
            "\n",
            "Finished in 1.18 seconds\n",
            "\n",
            "Total tokens in all completions: 1076\n",
            "## Step 1: Calculate the percentage\n",
            "To find the percentage, we need to divide the result by the base (360) and multiply by 100.\n",
            "\n",
            "## Step 2: Calculate the percentage value\n",
            "Percentage = (108 / 360) * 100 = 30%\n",
            "\n",
            "The final answer is: $\\boxed{30}$\n",
            "\n",
            "Finished in 1.25 seconds\n",
            "\n",
            "Total tokens in all completions: 1459\n",
            "## Step 1: Convert the speed of the train from km/hr to m/s\n",
            "To convert speed from km/hr to m/s, we multiply by 1000/3600.\n",
            "45 * (1000/3600) = 12.5 m/s\n",
            "\n",
            "## Step 2: Calculate the total distance the train needs to cover\n",
            "The total distance is the length of the train plus the length of the bridge.\n",
            "Total distance = 300 + 150 = 450 m\n",
            "\n",
            "## Step 3: Calculate the time it takes for the train to pass the bridge\n",
            "To calculate the time, we divide the total distance by the speed of the train.\n",
            "Time = Total distance / Speed\n",
            "Time = 450 / 12.5\n",
            "Time = 36 seconds\n",
            "\n",
            "The final answer is: $\\boxed{36}$\n",
            "\n",
            "Finished in 3.28 seconds\n",
            "\n",
            "Total tokens in all completions: 2691\n",
            "## Step 1: First, let's calculate 10^29.\n",
            "10^29 is a 1 followed by 29 zeros.\n",
            "\n",
            "## Step 2: Now, let's subtract 41 from 10^29.\n",
            "10^29 - 41 can be written as:\n",
            "1 followed by 29 zeros - 41\n",
            "= 000...00000000001 - 000...0000000041\n",
            "= 000...0000000049\n",
            "\n",
            "## Step 3: Finally, let's find the sum of the digits of the result from Step 2.\n",
            "The sum of the digits is 4 + 9 = 13.\n",
            "\n",
            "The final answer is: $\\boxed{13}$\n",
            "\n",
            "Finished in 1.26 seconds\n",
            "\n",
            "Total tokens in all completions: 1484\n",
            "## Step 1: Convert the speed of the train from km/hr to m/s\n",
            "First, we need to convert the speed of the train from km/hr to m/s because the length of the train will be in meters and time is in seconds. To convert km/hr to m/s, we multiply by 1000/3600.\n",
            "\n",
            "## Step 2: Calculate the speed of the train in m/s\n",
            "Speed in km/hr = 120 km/hr\n",
            "Speed in m/s = 120 * (1000/3600) = 33.33 m/s\n",
            "\n",
            "## Step 3: Use the formula distance = speed * time to find the length of the train\n",
            "Now that we know the speed of the train (33.33 m/s) and the time it takes to cross the pole (18 seconds), we can find the length of the train using the formula distance = speed * time.\n",
            "\n",
            "## Step 4: Calculate the length of the train\n",
            "Length of the train = speed * time = 33.33 m/s * 18 s = 597 m\n",
            "\n",
            "The final answer is: $\\boxed{597}$\n",
            "\n",
            "Finished in 1.55 seconds\n",
            "\n",
            "Total tokens in all completions: 1846\n",
            "## Step 1: Calculate the total distance the train travels to cross both the bridge and its own length.\n",
            "The total distance the train travels is the sum of its length and the length of the bridge, which is 100 meters + 300 meters = 400 meters.\n",
            "\n",
            "## Step 2: Calculate the speed of the train using the formula speed = distance / time.\n",
            "Given that the time taken is 45 seconds, the speed can be calculated as follows: speed = 400 meters / 45 seconds.\n",
            "\n",
            "## Step 3: Perform the division to find the speed in meters per second.\n",
            "speed = 400 / 45 = 8.89 meters per second.\n",
            "\n",
            "## Step 4: Convert the speed from meters per second to kilometers per hour.\n",
            "To convert meters per second to kilometers per hour, multiply by 3.6: speed = 8.89 meters/second * 3.6 = 32 kilometers/hour.\n",
            "\n",
            "The final answer is: $\\boxed{32}$\n",
            "\n",
            "Finished in 3.33 seconds\n",
            "\n",
            "Total tokens in all completions: 3670\n",
            "## Step 1: Calculate the current profit per item\n",
            "First, let's calculate the current profit per item. The retailer makes a profit of $40 on each item, but this profit constitutes 10% of the item's price to the retailer. Let's denote the item's price as P. Then, 10% of P is $40, so we can write 0.1P = $40.\n",
            "\n",
            "## Step 2: Solve for the item's price\n",
            "Now, we can solve for the item's price (P). We divide both sides of the equation by 0.1, giving P = $400.\n",
            "\n",
            "## Step 3: Calculate the current total profit per item\n",
            "The profit per item is $40, and the item's price is $400. Therefore, the current total profit per item is $40 + $400 = $440.\n",
            "\n",
            "## Step 4: Calculate the current total profit per month\n",
            "Each month, the retailer sells 100 items. So, the current total profit per month is $440 * 100 = $44,000.\n",
            "\n",
            "## Step 5: Calculate the profit needed after the discount\n",
            "If the retailer gives a 5% discount on the items they sell, they need to make a profit of $44,000 / 1.05 = $41,538 per month.\n",
            "\n",
            "## Step 6: Calculate the required number of items to be sold for the desired profit\n",
            "The profit per item is now $440, and the retailer needs a $41,538 profit after the discount. So, the required number of items to be sold is $41,538 / $440 = 94.64.\n",
            "\n",
            "## Step 7: Round up to the nearest whole number\n",
            "Since we can't sell a fraction of an item, we need to round up to the nearest whole number. Therefore, the retailer needs to sell at least 95 items per month to justify the policy of the discount.\n",
            "\n",
            "The final answer is: $\\boxed{95}$\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_1 = load_dataset(dataset_name, trust_remote_code=True)[split]\n",
        "# task = \"mathqa\"\n",
        "# Precompute outputs for a subset of examples (for demo purposes)\n",
        "with open(\"inputs.json\", 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "outputs = {}\n",
        "i = 0\n",
        "for example in dataset:\n",
        "    prompt = example[\"prompt\"]  # Input prompt from the dataset\n",
        "    formatted_output = generate_with_search_and_learn(question=prompt, config=config, llm=llm, prm=prm, method='best_of_n')\n",
        "    print(formatted_output)\n",
        "    outputs[prompt] = formatted_output\n",
        "    i += 1\n",
        "    if i > 100:\n",
        "        break\n",
        "    \n",
        "# # Save precomputed outputs to a JSON file\n",
        "# with open(\"precomputed_outputs_math_qa\", \"w\") as f:\n",
        "#     json.dump(outputs, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preparing_output_dataset(all_entries, all_options, outputs, dataset_name, split)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hack",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
