{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xctusmJ6BZ6_"
   },
   "source": [
    "# Scaling Test-Time Compute for Longer Thinking in LLMs\n",
    "\n",
    "_Adapted from [Hugging Face](https://github.com/huggingface/search-and-learn)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Requirements:**_ A100 GPU (good luck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook adapts the text-time compute solution presented in [this **blog post**](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) to extend its capabilities. The goal is to analyse inference-time compute and produce plots of number of generations vs quality. This should work for the two approaches for reasoning:\n",
    "* Verifier-based: plot number of generations versus number of tokens, to ultimately inform on the cost. Additionally, compute accuracy by connecting the results to nemo-evaluator\n",
    "* CoT-based: run deepseek-R1 in a recursive fashion (no verifier), using the distilled model \n",
    "\n",
    "This extension also allows to plug in different datasets for a quick understanding of the generalization capabilities of the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmgoppItAO7B"
   },
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/methods-thumbnail.png\" alt=\"Instruct LLM Methodology\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twKCzVIg71Xa"
   },
   "source": [
    "## 1. Install Dependencies _(copied from HF)_\n",
    "\n",
    "Let‚Äôs start by installing the [search-and-learn](https://github.com/huggingface/search-and-learn) repository! üöÄ  \n",
    "This repo is designed to replicate the experimental results and is not a Python pip package. However, we can still use it to generate our system. To do so, we‚Äôll need to install it from source with the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0YDC2_7XTm8"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/search-and-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd hack-search-and-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kT3jH_d_XcEb"
   },
   "outputs": [],
   "source": [
    "!pip install -e '.[dev]'\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAQHu9T176zh"
   },
   "source": [
    "Log in to Hugging Face to access [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as it is a gated model! üóùÔ∏è  \n",
    "If you haven't previously requested access, you'll need to submit a request before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pnEaTlFYZF_H"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4dd756970604521bb87666e3df00364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX07zCTA8MWL"
   },
   "source": [
    "## 2. Setup the Large Language Model (LLM) and the Process Reward Model (PRM) üí¨ _(copied from HF)_\n",
    "\n",
    "As illustrated in the diagram, the system consists of an LLM that generates intermediate answers based on user input, a [PRM model](https://huggingface.co/papers/2211.14275) that evaluates and scores these answers, and a search strategy that uses the PRM feedback to guide the subsequent steps in the search process until reaching the final answer.\n",
    "\n",
    "Let‚Äôs begin by initializing each model. For the LLM, we‚Äôll use the [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) model, and for the PRM, we‚Äôll use the [RLHFlow/Llama3.1-8B-PRM-Deepseek-Data](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkJw0x7gDJEY"
   },
   "source": [
    "![system](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/system.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_src = \"src/\"\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.append(project_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG1MolfxmZ7M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM\n",
    "from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "model_path=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "prm_path=\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
    "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
    "    seed=42,                     # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "prm = RLHFFlow(prm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYtPn0_V_YRx"
   },
   "source": [
    "### 2.1 Instantiate the Question, Search Strategy, and Call the Pipeline\n",
    "\n",
    "Now that we've set up the LLM and PRM, let's proceed by defining the question, selecting a search strategy to retrieve relevant information, and calling the pipeline to process the question through the models.\n",
    "\n",
    "1. **Instantiate the Question**: In this step, we define the input question that the system will answer, considering the given context.\n",
    "\n",
    "2. **Search Strategy**: The system currently supports the following search strategies: `best_of_n`, `beam_search`, and `dvts` (see diagram). For this example, we'll use `best_of_n`, but you can easily switch to any of the other strategies based on your needs. We need to define some configuration parameters for the configuration of the search strategy. You can check the full list [here](https://github.com/huggingface/search-and-learn/blob/main/src/sal/config.py).\n",
    "\n",
    "3. **Call the Pipeline**: With the question and search strategy in place, we‚Äôll call the inference pipeline, processing the inputs through both the LLM and PRM to generate the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSWINPerJrhm"
   },
   "source": [
    "![](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/search-strategies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z69xD6i2L5a6"
   },
   "source": [
    "The first step is to clearly define the question that the system will answer. This ensures that we have a precise task for the model to tackle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83puLxhzsOM0"
   },
   "outputs": [],
   "source": [
    "question_text = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
    "input_batch = {\"problem\": [question_text]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGpyzMNkAO7H"
   },
   "source": [
    "Next, we define the configuration, including parameters like the number of candidate answers `(N)`, and choose the search strategy that will be used. The search strategy dictates how we explore the potential answers. In this case, we'll use `best_of_n`.\n",
    "\n",
    "With the question and configuration in place, we use the selected search strategy to generate multiple candidate answers. These candidates are evaluated based on their relevance and quality and the final answer is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6s6GS16QZLV"
   },
   "outputs": [],
   "source": [
    "from sal.config import Config\n",
    "from sal.search import beam_search, best_of_n, dvts\n",
    "\n",
    "config = Config()\n",
    "config.n=32 # Number of answers to generate during the search\n",
    "\n",
    "search_result = best_of_n(x=input_batch, config=config, llm=llm, prm=prm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsLHD_6C_15p"
   },
   "source": [
    "### 2.2 Display the Final Result\n",
    "\n",
    "Once the pipeline has processed the question through the LLM and PRM, we can display the final result. This result will be the model's output after considering the intermediate answers and scoring them using the PRM.\n",
    "\n",
    "Here's how to display the final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "v8medbURbgdI",
    "outputId": "3620f3e6-a25d-4bec-f41c-c4f03a6ed770"
   },
   "outputs": [],
   "source": [
    "search_result['pred'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-8hIu05AO7J"
   },
   "source": [
    "The model‚Äôs output might include special tokens, such as `<|start_header_id|>` or `<|end_header_id|>`. To make the answer more readable, we can safely remove them before displaying it to the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "flbIu6-rDapM",
    "outputId": "fcb197d5-0f21-4953-8a21-869c92a1f957"
   },
   "outputs": [],
   "source": [
    "formatted_output = search_result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
    "formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZuLZNirAO7J"
   },
   "source": [
    "After removing any special tokens, we can display the final answer to the user. Since the answer is based on markdown, it can be rendered properly by displaying it as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "P4En0qJRD0cl",
    "outputId": "56400fea-e304-4f16-d255-909f42f636e0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uCpYzAw_4o9"
   },
   "source": [
    "## 3. Assembling It All! üßë‚Äçüè≠Ô∏è _(copied from HF)_\n",
    "\n",
    "Now, let's create a method that encapsulates the entire pipeline. This will allow us to easily reuse the process in future applications, making it efficient and modular.\n",
    "\n",
    "By combining the LLM, PRM, search strategy, and result display, we can simplify the workflow and ensure that it‚Äôs reusable for other tasks or questions.\n",
    "\n",
    "We simplify the workflow, ensuring that it‚Äôs reusable for different tasks or questions. Additionally, we‚Äôll track the time spent on each method so that we can **understand the practical implications** of using each strategy and configuration.\n",
    "\n",
    "Here‚Äôs how we can structure the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpswbcVi37KR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given question using the search-and-learn pipeline.\n",
    "\n",
    "    Args:\n",
    "    - question (str): The input question to generate an answer for.\n",
    "    - config (Config): Configuration object containing parameters for search strategy.\n",
    "    - llm (LLM): Pretrained large language model used for generating answers.\n",
    "    - prm (RLHFFlow): Process reward model used for evaluating answers.\n",
    "    - method (str): Search strategy to use. Options are 'best_of_n', 'beam_search', 'dvts'. Default is 'best_of_n'.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted output after processing the question.\n",
    "    \"\"\"\n",
    "    batch = {\"problem\": [question]}\n",
    "\n",
    "    start_time = time.time()\n",
    "    if method == 'best_of_n':\n",
    "      result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n",
    "    elif method == 'beam_search':\n",
    "      result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n",
    "    elif method == 'dvts':\n",
    "      result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    total_tokens = 0\n",
    "    for completion in result['completions']:\n",
    "        for comp in  completion:\n",
    "            output_tokens = tokenizer.encode(comp)\n",
    "            total_tokens += len(output_tokens)\n",
    "\n",
    "    print(f\"Total tokens in all completions: {total_tokens}\")\n",
    "\n",
    "    formatted_output = result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
    "    return formatted_output, elapsed_time, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWbOqkiKPVd2"
   },
   "source": [
    "### ‚è≥  3.1 Comparing Thinking Time for Each Strategy\n",
    "\n",
    "Let‚Äôs compare the **thinking time** of three methods: `best_of_n`, `beam_search`, and `dvts`. Each method is evaluated using the same number of answers during the search process, measuring the time spent thinking in seconds and the number of generated tokens.\n",
    "\n",
    "In the results below, the `best_of_n` method shows the least thinking time, while the `dvts` method takes the most time. However, `best_of_n` generates more tokens due to its simpler search strategy.\n",
    "\n",
    "| **Method**      | **Number of Answers During Search** | **Thinking Time (Seconds)** | **Generated Tokens** |\n",
    "|------------------|-------------------------------------|-----------------------------|-----------------------|\n",
    "| **best_of_n**    | 8                                   | 3.54                        | 3087                  |\n",
    "| **beam_search**  | 8                                   | 10.06                       | 2049                  |\n",
    "| **dvts**         | 8                                   | 8.46                        | 2544                  |\n",
    "\n",
    "This comparison illustrates the trade-offs between the strategies, balancing time spent thinking and the complexity of the search process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ROJwROGX8q-"
   },
   "source": [
    "#### 1. **Best of n**\n",
    "\n",
    "We‚Äôll begin by using the `best_of_n` strategy. Here‚Äôs how to track the thinking time for this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_fWKy5CCTLV",
    "outputId": "8d77eea3-b23e-4eba-cfe3-5935fae1405d"
   },
   "outputs": [],
   "source": [
    "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
    "\n",
    "config.n=8\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "uzKfFoKG9ejC",
    "outputId": "38326907-685e-4a9c-ca8b-32a7c40f1d3e"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S9AwP5lQvUN"
   },
   "source": [
    "#### 2. **Beam Search**\n",
    "\n",
    "Now, let's try using the `beam_search` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7CH6KN8Izp9",
    "outputId": "adef4782-3278-4994-9520-43e23ea047a6"
   },
   "outputs": [],
   "source": [
    "config.n=8\n",
    "# beam search specific\n",
    "config.sort_completed=True\n",
    "config.filter_duplicates=True\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='beam_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "Hw6tQD_dMwXZ",
    "outputId": "0f66c7ed-2071-45a4-e562-3967deb0bc9d"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxBBUd7HQzhd"
   },
   "source": [
    "#### 3. **Diverse Verifier Tree Search (DVTS)**\n",
    "\n",
    "Finally, let's try the `dvts` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzXW1g-dI5wN",
    "outputId": "86979d67-7dfa-4346-9adb-c386a52af58c"
   },
   "outputs": [],
   "source": [
    "config.n=8\n",
    "# dvts specific\n",
    "config.n_beams = config.n // config.beam_width\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='dvts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "RGkG9MPXMvN0",
    "outputId": "18a333ae-7b3a-455e-df2c-bb497b1381a5"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PM9HHwBSYWk"
   },
   "source": [
    "### üôã 3.2 Testing the System with a Simple Question\n",
    "\n",
    "In this final example, we‚Äôll test the system using a straightforward question to observe how it performs in simpler cases. This allows us to verify that the system works as expected even for basic queries.\n",
    "\n",
    "Let's try the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq9vM1uRM7A8",
    "outputId": "65ef318d-2b89-4d46-b660-293195c2b8e1"
   },
   "outputs": [],
   "source": [
    "question = 'What\\'s the capital of Spain?'\n",
    "\n",
    "config.n=32\n",
    "\n",
    "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "ysfR0nPfM-Ub",
    "outputId": "b474aeb6-6cb7-4f15-ba48-fa59022f31ef"
   },
   "outputs": [],
   "source": [
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgdeSegeANoT"
   },
   "source": [
    "Even though we set a larger number of candidate answers (`N`), the time spent thinking remains relatively small (1.03 seconds and 544 generated tokens). This demonstrates the system‚Äôs ability to efficiently handle easier problems, spending less time on them, while leveraging its enhanced capabilities for more complex questions.\n",
    "\n",
    "üèÜ **We now have a fully operational pipeline** that leverages test-time compute, enabling the system to \"think longer\" for more complicated queries, while also maintaining fast response times for straightforward questions.\n",
    "\n",
    "This approach ensures the system can scale its thinking time based on the task's complexity, offering an efficient and responsive solution for both simple and challenging problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking _(extension by NVIDIA)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Defining what we are evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "prm_path = \"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Methods to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods\n",
    "methods = [\"Best-of-n\", \"Beam search\", \"Diverse verifier tree search\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16, 32, 64, 128, 256]\n"
     ]
    }
   ],
   "source": [
    "# Which n values are being tested\n",
    "\n",
    "n_values = [2**i for i in range(2, 9)]\n",
    "print(n_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"allenai/math_qa\"\n",
    "split = \"test\"\n",
    "samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the model using vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM\n",
    "from sal.models.reward_models import RLHFFlow\n",
    "from sal.search import beam_search, best_of_n, dvts\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_src = \"src/\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
    "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
    "    seed=42,                     # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "prm = RLHFFlow(prm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generating all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/hack-search-and-learn/evaluation/hack-eval/prep.py\"))\n",
    "from prep import preparing_input_dataset, preparing_output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting questions from the dataset (all_entries)\n",
    "all_entries, all_options = preparing_input_dataset(dataset_name, split, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "# Add it to sys.path\n",
    "sys.path.append(project_src)\n",
    "from sal.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = {}\n",
    "\n",
    "# Define output filename for tokens and time\n",
    "csv_filename = \"search_methods_results.csv\"\n",
    "\n",
    "# Define headers (Each method has its own Time (s) and Total tokens)\n",
    "headers = [\"Number of generations\"]\n",
    "\n",
    "for method in methods:\n",
    "    headers.append(f\"{method} Time (s)\")\n",
    "    headers.append(f\"{method} Total tokens\")\n",
    "    \n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)  # Write header\n",
    "\n",
    "    for i in n_values:\n",
    "        config = Config()\n",
    "        config.n = i\n",
    "        row = [i]\n",
    "        \n",
    "        for method in methods:\n",
    "\n",
    "            if method == \"Beam search\":\n",
    "                config.sort_completed=True\n",
    "                config.filter_duplicates=True\n",
    "            elif method == \"Diverse verifier tree search\":\n",
    "                config.n_beams = config.n // config.beam_width\n",
    "\n",
    "            method_output = {}\n",
    "            for n, example in enumerate(all_entries):\n",
    "                prompt = example[\"prompt\"]  # Input prompt from the dataset\n",
    "                formatted_output, elapsed_time, token_number = generate_with_search_and_learn(question=prompt, config=config, llm=llm, prm=prm, method=method)\n",
    "                \n",
    "                if n == 0:\n",
    "                    row.append(elapsed_time)\n",
    "                    row.append(token_number)\n",
    "                    \n",
    "                \n",
    "                method_output[prompt] = formatted_output\n",
    "\n",
    "            all_outputs.update({method + \"_\" + i: method_output})\n",
    "            \n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV file '{csv_filename}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Computing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates a {method}_outputs.json file that can be ingested by nemo evaluator\n",
    "\n",
    "# You need to choose the evaluation first\n",
    "\n",
    "results_to_evaluate = all_outputs[\"Best-of-n_4\"]\n",
    "\n",
    "preparing_output_dataset(all_entries, all_options, results_to_evaluate, dataset_name, split, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<< Offline connection to NEMO Evaluator >>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluation: Token / Time Versus Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_methods(csv_file, analysis_type=\"Time (s)\"): # or Total tokens\n",
    "\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    col_names = df.columns\n",
    "\n",
    "    # Extract relevant columns\n",
    "    x = df[col_names[0]] # Should be Number of Generations\n",
    "    number_of_methods = len(col_names) // 2\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if \"Time\" in analysis_type:\n",
    "        start = 1\n",
    "        markers = ['o', 'o', 'o']\n",
    "    else: # Tokens\n",
    "        start = 2\n",
    "        markers = ['o', 's', '^']\n",
    "\n",
    "    j = 0\n",
    "    for i in range(start, len(col_names), 2):\n",
    "        plt.plot(x, df[col_names[i]], marker=markers[j], label=col_names[i].split(analysis_type)[0])\n",
    "        j += 1\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(col_names[0])\n",
    "    plt.ylabel(analysis_type)\n",
    "\n",
    "    plt.suptitle(f\"Elapsed {analysis_type} in All Completions vs. Number of Generations\", fontsize=14)\n",
    "    plt.title(f\"LLM: {model_path}, PRM: {prm_path}\", fontsize=10, color='gray')\n",
    "    plt.legend()\n",
    "\n",
    "    # Set log scale\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    # Ensure x-axis ticks show as integers\n",
    "    plt.xticks(x, labels=[str(int(val)) for val in x])  \n",
    "\n",
    "    # Grid and formatting\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "csv_filename = \"search_methods_results.csv\"  # Update this with the actual CSV file path\n",
    "\n",
    "plot_methods(csv_filename, \"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_methods(csv_filename, \"Total tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Llama 1b and Llama 3b on the basis of DVTS Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "llama1b_csv = \"llama_3_2_1b.csv\"  \n",
    "llama3b_csv = \"llama_3_2_3b.csv\"  \n",
    "llama8b_csv = \"llama_3_1_8b.csv\"\n",
    "\n",
    "df_1b = pd.read_csv(llama1b_csv)\n",
    "df_3b = pd.read_csv(llama3b_csv)\n",
    "df_8b = pd.read_csv(llama8b_csv)\n",
    "\n",
    "# Extract relevant data\n",
    "x_1b = df_1b[\"Number of generations\"]\n",
    "time_1b = df_1b[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "x_3b = df_3b[\"Number of generations\"]\n",
    "time_3b = df_3b[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "x_8b = df_8b[\"Number of generations\"]\n",
    "time_8b = df_8b[\"Diverse verifier tree search Time (s)\"]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_1b, time_1b, marker='o', label=\"Llama 3.2 1b\", linestyle='-')\n",
    "plt.plot(x_3b, time_3b, marker='s', label=\"Llama 3.2 3b\", linestyle='--')\n",
    "plt.plot(x_3b, time_3b, marker='x', label=\"Llama 3.1 8b\", linestyle='---')\n",
    "\n",
    "# Titles and Labels\n",
    "plt.suptitle(\"Comparison of Diverse Verifier Tree Search Time\", fontsize=14)\n",
    "plt.title(\"Llama 3.2 1b vs. Llama 3.2 3b vs Llama 3.1 8b\", fontsize=10, color='gray')\n",
    "plt.xlabel(\"Number of Generations\")\n",
    "plt.ylabel(\"Diverse Verifier Tree Search Time (s)\")\n",
    "plt.legend()\n",
    "\n",
    "# Ensure integer values on x-axis\n",
    "plt.xticks(x_1b, labels=[str(int(val)) for val in x_1b])  \n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Llama 1b and Llama 3b on the basis of DVTS Tokens number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "llama1b_csv = \"llama_3_2_1b.csv\"  \n",
    "llama3b_csv = \"llama_3_2_3b.csv\"  \n",
    "llama8b_csv = \"llama_3_1_8b.csv\"\n",
    "\n",
    "df_1b = pd.read_csv(llama1b_csv)\n",
    "df_3b = pd.read_csv(llama3b_csv)\n",
    "df_8b = pd.read_csv(llama8b_csv)\n",
    "\n",
    "# Extract relevant data\n",
    "x_1b = df_1b[\"Number of generations\"]\n",
    "tokens_1b = df_1b[\"Diverse verifier tree search Total tokens\"].astype(int)\n",
    "\n",
    "x_3b = df_3b[\"Number of generations\"]\n",
    "tokens_3b = df_3b[\"Diverse verifier tree search Total tokens\"].astype(int)\n",
    "\n",
    "x_8b = df_8b[\"Number of generations\"]\n",
    "tokens_8b = df_8b[\"Diverse verifier tree search Total tokens\"].astype(int)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_1b, tokens_1b, marker='o', label=\"Llama 1b\", linestyle='-')\n",
    "plt.plot(x_3b, tokens_3b, marker='s', label=\"Llama 3b\", linestyle='--')\n",
    "plt.plot(x_8b, tokens_8b, marker='x', label=\"Llama 8b\", linestyle='--')\n",
    "\n",
    "# Titles and Labels\n",
    "plt.suptitle(\"Comparison of Total Tokens in Diverse Verifier Tree Search\", fontsize=14)\n",
    "plt.title(\"Llama 3.2 1b vs. Llama 3.2 3b vs Llama 3.1 8b\", fontsize=10, color='gray')\n",
    "plt.xlabel(\"Number of Generations\")\n",
    "plt.ylabel(\"Total Tokens\")\n",
    "plt.legend()\n",
    "\n",
    "# Ensure integer values on x-axis\n",
    "plt.xticks(x_1b, labels=[str(int(val)) for val in x_1b])  \n",
    "\n",
    "# Ensure integer values on y-axis\n",
    "y_ticks = sorted(set(tokens_1b.tolist() + tokens_3b.tolist()))\n",
    "plt.yticks(y_ticks, labels=[str(val) for val in y_ticks])\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
